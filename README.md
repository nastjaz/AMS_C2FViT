# AMS IZZIV - final report
Nastja ≈Ωgalin

Affine Medical Image Registration with Coarse-to-Fine Vision Transformer (C2FViT)

https://github.com/nastjaz/AMS_C2FViT.git

## Method Explanation
This is the official Pytorch implementation of "Affine Medical Image Registration with Coarse-to-Fine Vision Transformer" (CVPR 2022), written by Tony C. W. Mok and Albert C. S. Chung.

![plot](./source/Figure/overview.png?raw=true)

The Vision Transformer (ViT) learns the optimal affine transformation matrix to align fixed and moving grayscale images. The C2FViT model employs a multistage pipeline with convolutional patch embeddings and transformer encoder blocks to extract hierarchical features at progressively finer scales. Locality is enhanced through convolutional patch embeddings and depth-wise convolutions in feed-forward layers, while global connectivity is achieved via multi-head self-attention mechanisms.

To enhance flexibility and generalizability in affine image registration, C2FViT predicts geometric transformation parameters‚Äîtranslation, rotation, scaling, and shearing‚Äîinstead of directly estimating the affine matrix. The affine matrix is derived as the product of these geometric transformation matrices, enabling seamless adaptation to other parametric registration methods, such as rigid registration, by omitting scaling and shearing components. Geometrical constraints reduce the search space, with rotation and shearing parameters restricted to [-œÄ, +œÄ], translation constrained to ¬±50% of spatial resolution, and scaling limited to [0.5, 1.5]. For rotation and shearing, the center of mass is used as the origin instead of the geometric center.

In the unsupervised learning paradigm, the model minimizes the dissimilarity between the fixed image ùêπ and the warped moving image ùëÄ(ùúë(ùê¥ùëì)), where ùê¥ùëì is the affine transformation matrix predicted by the model and ùúë represents the spatial transformation function. 
The similarity measure is based on the negative normalized cross-correlation (NCC), ensuring robust alignment between ùêπ and ùëÄ(ùúë(ùê¥ùëì)). 


## Results

```bash
aggregated_results:
        LogJacDetStd        : 0.00000 +- 0.00000 | 30%: 0.00000
        TRE_kp              : 12.12685 +- 2.19995 | 30%: 13.76419
        TRE_lm              : 12.12293 +- 3.21796 | 30%: 13.88543
        DSC                 : 0.24743 +- 0.07594 | 30%: 0.18039
        HD95                : 47.75687 +- 10.91122 | 30%: 38.20628
```

![plot](./source/Figure/0011.png?raw=true)

## Docker Information

First command builds a Docker image named my-docker-image from a Dockerfile in the current directory (AMS_C2FViT).

docker build -t my-docker-image -f Dockerfile .


Second command runs a container based on this image to execute a Python script with specific arguments for model inference in an image registration task.

docker run --name {name of a container} --runtime=nvidia -it --rm -v $(pwd):/workdir --workdir {workdir} python3 Test_C2FViT_pairwise.py --modelpath {model_path} --fixed {fixed_img_path} --moving {moving_img_path}

### Example
docker run --name new-container --runtime=nvidia -it --rm -v $(pwd):/workdir --workdir /workdir/source my-docker-image python3 Code/Test_C2FViT_pairwise.py --modelpath Model/CBCT_affineC2FViT_1000stagelvl3_0.pth --fixed Data/validation/ThoraxCBCT_0011_0001.nii.gz --moving Data/validation/ThoraxCBCT_0011_0000.nii.gz

## Data Preparation
Explain the steps required to prepare the data for training. Include any preprocessing steps and data splitting.

## Train Commands
If applicable, list the commands needed to train your model. Provide any necessary explanations or parameters. 
For train.py script, you should use a parser to set all input parameters. Below is the example, how to run `train.py`:





`

Pairwise image registration:

`python Test_C2FViT_pairwise.py --modelpath {model_path} --fixed {fixed_img_path} --moving {moving_img_path}`


## Pre-trained model weights
Pre-trained model weights can be downloaded with the links below:

Unsupervised:
- [C2FViT_affine_COM_pairwise_stagelvl3_118000.pth](https://drive.google.com/file/d/1CQvyx96YBor9D7TWvvqHs6fuiJl-Jfay/view?usp=sharing)
- [C2FViT_affine_COM_template_matching_stagelvl3_116000.pth](https://drive.google.com/file/d/1uIItkfByyDYtxVxsjems_1HATRzcVCWX/view?usp=sharing)

Semi-supervised:
- [C2FViT_affine_COM_pairwise_semi_stagelvl3_95000.pth](https://drive.google.com/file/d/1T5JvXa3dCkFoFXNe5k7m3TDVn9AJv2_H/view?usp=sharing)
- [C2FViT_affine_COM_template_matching_semi_stagelvl3_130000.pth](https://drive.google.com/file/d/1bfh_jVOK5Ip2bBuTpCPYlQGFCMWpG_cb/view?usp=sharing)

## Train your own model
Step 0 (optional): Download the preprocessed OASIS dataset from https://github.com/adalca/medical-datasets/blob/master/neurite-oasis.md and place it under the `Data` folder.

Step 1: Replace `/PATH/TO/YOUR/DATA` with the path of your training data, e.g., `../Data/OASIS`, and make sure `imgs` and `labels` are properly loaded in the training script.

Step 2: Run `python {training_script}`, see "Training and testing scripts" for more details.

## Publication
If you find this repository useful, please cite:
- **Affine Medical Image Registration with Coarse-to-Fine Vision Transformer**  
[Tony C. W. Mok](https://cwmok.github.io/ "Tony C. W. Mok"), Albert C. S. Chung  
CVPR2022. [eprint arXiv:2203.15216](https://arxiv.org/abs/2203.15216)


## Acknowledgment
Some codes in this repository are modified from [PVT](https://github.com/whai362/PVT) and [ViT](https://github.com/lucidrains/vit-pytorch).
The MNI152 brain template is provided by the [FLIRT (FMRIB's Linear Image Registration Tool)](https://fsl.fmrib.ox.ac.uk/fsl/fslwiki/FLIRT#Template_Images).

###### Keywords
Keywords: Affine registration, Coarse-to-Fine Vision Transformer, 3D Vision Transformer